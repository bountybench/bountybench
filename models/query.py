from typing import List

import tiktoken

from models.helm_models.helm_models import HelmModels
from models.model_provider import ModelProvider
from models.openai_models.openai_models import OpenAIModels
from models.openai_models.azure_openai_models import AzureOpenAIModels
from models.model_mapping import ModelRegistry 
from models.model_response import ModelResponse
from utils.workflow_logger import workflow_logger



def get_model_provider(model: str, helm: bool = False) -> ModelProvider:
    """
    Get the appropriate model provider based on the model type.
    Args:
        model (str): The model name.
        helm (bool): Flag indicating whether to use HelmModels. Defaults to False.
    Returns:
        ModelProvider: An instance of the appropriate model provider class.
    """
    # TODO: Support Different Model Providers (Also handle Azure case)
    if helm:
        model_provider = HelmModels()
    else:
        model_provider = OpenAIModels()
    return model_provider


def tokenize(model: str, message: str) -> List[int]:
    """
    Tokenize the given message using the specified model's tokenizer.
    Args:
        model (str): The model name to use for tokenization.
        message (str): The message to tokenize.
    Returns:
        List[int]: A list of token IDs representing the tokenized message.
    """
    model_provider: ModelProvider
    model_provider = get_model_provider(model)
    try:
        return model_provider.tokenize(
            model=model, message=message
        )
    except (NotImplementedError, KeyError):
        encoding = tiktoken.encoding_for_model("gpt-4")
        return encoding.encode(message)


def decode(model: str, tokens: List[int]) -> str:
    """
    Decode a list of token IDs back into a string using the specified model's tokenizer.
    Args:
        model (str): The model name to use for decoding.
        tokens (List[int]): A list of token IDs to decode.
    Returns:
        str: The decoded string representation of the tokens.
    """
    model_provider: ModelProvider
    model_provider = get_model_provider(model)
    try:
        return model_provider.decode(
            model=model, tokens=tokens
        )
    except (NotImplementedError, KeyError):
        encoding = tiktoken.encoding_for_model("gpt-4")
        return encoding.decode(tokens)


async def query(
    model: str,
    message: str,
    temperature: float,
    max_tokens: int,
    stop_sequences: List[str],
    helm: bool,
) -> ModelResponse:
    """
    Send a query to the specified model and get a response.
    Args:
        model (str): The model name to use for the query.
        message (str): The input message to send to the model.
        temperature (float): Controls the creativity of the response.
        max_tokens (int): The maximum number of tokens to generate in the response.
        stop_sequences (List[str]): Sequences that will stop the model's response.
        helm (bool): Flag indicating whether to use HelmModels. Defaults to False.
    Returns:
        str: The response generated by the model.
    """
    model_provider: ModelProvider
    model_provider = get_model_provider(model=model, helm=helm)
    model_response = model_provider.request(
        model=model,
        message=message,
        temperature=temperature,
        max_tokens=max_tokens,
        stop_sequences=stop_sequences,
    )
    await workflow_logger.log_action(
        action_name="llm",
        input_data=message,
        output_data=model_response.content,
        metadata={
            "model": model,
            "temperature": temperature,
            "max_input_tokens": max_tokens,
            "stop_sequences": stop_sequences,
            "input_tokens": model_response.input_tokens,
            "output_tokens": model_response.output_tokens,
            "time_taken_in_ms": model_response.time_taken_in_ms,
        },
    )
    return model_response